= Example for OpenMetaData with Confluent Platform

This is an example how OpenMetaData can be integrated with Confluent Platform.

This example code consists of a transactional producer, a simple Kafka Streams application which just forwards data to another topic, and a transactional consumer.

DISCLAIMER: This project is for demonstration purposes only. Using the concept in production is highly discouraged. Use at your own risk.

== Preconditions

This project has been tested with Java version 17 and Gradle version TODO

== Running OpenMetaData and Confluent Platform
Change to folder `cluster`.

Start the containers by running:
```
docker-compose up -d
```

Stopping the containers:
```
docker-compose down
```

Cleaning up (CAREFUL: THIS WILL DELETE ALL UNUSED VOLUMES):
```
docker volumes prune
```

== Create topics with one partition

```shell
docker exec broker kafka-topics --bootstrap-server localhost:9092 --create --topic topic-1 --partitions 1
docker exec broker kafka-topics --bootstrap-server localhost:9092 --create --topic topic-2 --partitions 1
```


== Accessing the Web UIs

The OpenMetaData Web UI is located here: http://localhost:8585.
Username: admin
Password: admin

The Confluent Control Center is located here: http://localhost:9021.

== Configuring OpenMetaData

Log into OpenMetaData as `admin` user. In the settings, add a messaging service of type `kafka`:
Use these values:

Bootstrap server: localhost:9092
Schema Registry URL: localhost:8081

== Building the Producers
Initialize by running
```
gradle wrapper
```

Build Jar including all libraries with:
```
./gradlew shadowJar
```

== Running the transactional producer
Change to the "clients" folder. Then use the following commands to run the code:

```
./gradlew -p producer-transactional run
```

Alternatively, run the combined jar like this:

```shell
java -jar producer-transactional/build/libs/producer-transactional-all.jar producer-transactional.properties
```

== Running the transactional consumer
Change to the "clients" folder. Then use the following commands to run the code:

```
./gradlew -p consumer-transactional run
```

== Running the transactional Kafka streams application
Change to the "clients" folder. Then use the following commands to run the code:

```
./gradlew -p kstreams-header-forward run
```

This will forward all commited transactional messages and non-transactional messages from `topic-1` to `topic-2`.
There are three different, very simple topologies in the implementation which have in common that they will retain the original context including the headers send with the transactional messages (in this example, this would work for non-transactional messages, too, but here we do not add headers to those).

== Some helpful commands
Consume from the topic like this, including the headers of the messages:

```shell
kafka-console-consumer --bootstrap-server localhost:9092 \
    --from-beginning \
    --property print.headers=true \
    --topic topic-1
```

Note that the above command will also show uncommited messages. Thus, if you want to see only commited messages, use the following command instead:

```shell
kafka-console-consumer --bootstrap-server localhost:9092 \
    --from-beginning \
    --isolation-level=read_committed \
    --property print.headers=true \
    --topic topic-1
```

You can delete the auto-created topic like this:

```shell
kafka-topics --bootstrap-server localhost:9092 --delete --topic topic-1
```

== Results

If mixing transactional and non-transactional messages in a single topic, consumers will still see all messages by default. With `isolation.level=read_committed` they will just see all commited messages from the transactions and additionally all non-transactional messages. The messages are filtered on the consumer side, but this happens inside of the Kafka client library and is not exposed to the customer application. Particularly, it is not possible on the application level to distinguish between commited transactional messages and non-transactional messages.

