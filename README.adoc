= Example for OpenMetaData with Confluent Platform

This is an example how OpenMetaData can be integrated with Confluent Platform.

This example code consists of a transactional producer, a simple Kafka Streams application which just forwards data to another topic, and a transactional consumer.

DISCLAIMER: This project is for demonstration purposes only. Using the demo unmodified in production is highly discouraged. Use at your own risk.

== Preconditions

This project has been tested with Java version 17 and Gradle version TODO

== Running OpenMetaData and Confluent Platform
Change to folder `cluster`.

Start the containers by running:
```shell
docker compose up -d
```

Note: If you have a large machine with many cores and lots of RAM, you can optionally run Confluent Control center
```shell
docker compose --profile c3 up -d
```


Stopping the containers (but leave volumes intact):
```shell
docker compose down
```

Stopping the containers (and delete all volumes, CAREFUL: THIS WILL DELETE ALL UNUSED VOLUMES):
```shell
docker compose down -v
```

Cleaning up, if you haven't done that already (CAREFUL: THIS WILL DELETE ALL UNUSED VOLUMES):
```shell
docker volumes prune
```

== Automatic setup
Everything should have been initialized already, including topic creation ("topic-1" and "topic-2").

== Manual setup

=== Create topics with one partition

```shell
docker exec broker kafka-topics --bootstrap-server localhost:9092 --create --topic topic-1 --partitions 1
docker exec broker kafka-topics --bootstrap-server localhost:9092 --create --topic topic-2 --partitions 1
```

==== Initialize schema registry

First, set the compatilibity level of our subject to "FORWARD_TRANSITIVE":

```bash
docker exec broker curl -X PUT -H "Content-Type: application/vnd.schemaregistry.v1+json" \
--data '{"compatibility": "FORWARD_TRANSITIVE"}' \
http://schema-registry:8081/config/topic-1-value
docker exec broker curl -X PUT -H "Content-Type: application/vnd.schemaregistry.v1+json" \
--data '{"compatibility": "FORWARD_TRANSITIVE"}' \
http://schema-registry:8081/config/topic-2-value
```


Register the schema:

```bash
docker exec broker curl -X POST -H "Content-Type: application/vnd.schemaregistry.v1+json" \
--data '{"schema": "{\"namespace\": \"models.avro\", \"type\": \"record\", \"name\": \"SimpleValue\", \"fields\": [ {\"name\": \"theName\", \"type\": \"string\"}, {\"name\": \"theValue\", \"type\": \"string\"}]}"}' \
http://schema-registry:8081/subjects/topic-1-value/versions
docker exec broker curl -X POST -H "Content-Type: application/vnd.schemaregistry.v1+json" \
--data '{"schema": "{\"namespace\": \"models.avro\", \"type\": \"record\", \"name\": \"SimpleValue\", \"fields\": [ {\"name\": \"theName\", \"type\": \"string\"}, {\"name\": \"theValue\", \"type\": \"string\"}]}"}' \
http://schema-registry:8081/subjects/topic-2-value/versions
```

Try to register an updated version of the schema, which is NOT forward compatible (should fail!):

```bash
docker exec broker curl -X POST -H "Content-Type: application/vnd.schemaregistry.v1+json" \
--data '{"schema": "{\"namespace\": \"models.avro\", \"type\": \"record\", \"name\": \"SimpleValue\", \"fields\": [ {\"name\": \"theName\", \"type\": \"string\"}]}"}]}"}' \
http://schema-registry:8081/subjects/topic-1-value/versions
```


Register an updated version of the schema, which is forward compatible (thus should in theory allow an older consumer to read it). We just add a another value:

```bash
docker exec broker curl -X POST -H "Content-Type: application/vnd.schemaregistry.v1+json" \
--data '{"schema": "{\"namespace\": \"models.avro\", \"type\": \"record\", \"name\": \"SimpleValue\", \"fields\": [ {\"name\": \"theName\", \"type\": \"string\"}, {\"name\": \"theValue\", \"type\": \"string\"}, {\"name\": \"theNewName\", \"type\": \"string\"}]}"}' \
http://schema-registry:8081/subjects/topic-1-value/versions
docker exec broker curl -X POST -H "Content-Type: application/vnd.schemaregistry.v1+json" \
--data '{"schema": "{\"namespace\": \"models.avro\", \"type\": \"record\", \"name\": \"SimpleValue\", \"fields\": [ {\"name\": \"theName\", \"type\": \"string\"}, {\"name\": \"theValue\", \"type\": \"string\"}, {\"name\": \"theNewName\", \"type\": \"string\"}]}"}' \
http://schema-registry:8081/subjects/topic-2-value/versions
```


== Accessing the Web UIs

The OpenMetaData Web UI is located here: http://localhost:8585.

* Username: admin
* Password: admin

The Airflow Web UI is located here: http://localhost:8080.

* Username: admin
* Password: admin

The Confluent Control Center is located here: http://localhost:9021.

== Configuring OpenMetaData manually

Log into OpenMetaData as `admin` user. In the settings, add a messaging service of type `kafka`:
Use these values:

Bootstrap server: broker:9092
Schema Registry URL: http://schema-registry:8081

An ingestion for the service for the Kafka cluster is created automatically at startup by this demo.

You should see the imported topics and their schemas.

== Building the Producers
Initialize by running
```
gradle wrapper
```

Build Jar including all libraries with:
```
./gradlew shadowJar
```

== Running the transactional producer
Change to the "clients" folder. Then use the following commands to run the code:

```
./gradlew -p producer-transactional run
```

Alternatively, run the combined jar like this:

```shell
java -jar producer-transactional/build/libs/producer-transactional-all.jar producer-transactional.properties
```

== Running the transactional consumer
Change to the "clients" folder. Then use the following commands to run the code:

```
./gradlew -p consumer-transactional run
```

== Running the transactional Kafka streams application
Change to the "clients" folder. Then use the following commands to run the code:

```
./gradlew -p kstreams-header-forward run
```

This will forward all commited transactional messages and non-transactional messages from `topic-1` to `topic-2`.
There are three different, very simple topologies in the implementation which have in common that they will retain the original context including the headers send with the transactional messages (in this example, this would work for non-transactional messages, too, but here we do not add headers to those).

== Some helpful commands
Consume from the topic like this, including the headers of the messages:

```shell
kafka-console-consumer --bootstrap-server localhost:9092 \
    --from-beginning \
    --property print.headers=true \
    --topic topic-1
```

Note that the above command will also show uncommited messages. Thus, if you want to see only commited messages, use the following command instead:

```shell
kafka-console-consumer --bootstrap-server localhost:9092 \
    --from-beginning \
    --isolation-level=read_committed \
    --property print.headers=true \
    --topic topic-1
```

== Inserting data into MariaDB SQL

Run mysql like this:
```shell
docker compose exec mariadb mariadb -u root -pmariadb demo
```

Insert some data:
```SQL
INSERT INTO Persons (LastName, FirstName, Address, City) VALUES ("Doe", "John", "Unknown", "Unknown");
INSERT INTO Persons (LastName, FirstName, Address, City) VALUES ("Doe", "Jane", "Unknown", "Unknown");
```

== Helpful commands

You can delete the topics like this:

```shell
kafka-topics --bootstrap-server localhost:9092 --delete --topic topic-1
kafka-topics --bootstrap-server localhost:9092 --delete --topic topic-2
```
